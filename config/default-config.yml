config-file: "./default-config.yml"
network-config:
  # Network Configuration
  # Connection pruning determines whether connections to nodes
  # that are not part of protocol state should be trimmed
  networking-connection-pruning: true
  # Preferred unicasts protocols list of unicast protocols in preferred order
  preferred-unicast-protocols: [ ]
  received-message-cache-size: 10_000
  peerupdate-interval: 10m
  unicast-message-timeout: 5s
  # Unicast create stream retry delay is initial delay used in the exponential backoff for create stream retries
  unicast-create-stream-retry-delay: 1s
  dns-cache-ttl: 5m
  # The size of the queue for notifications about new peers in the disallow list.
  disallow-list-notification-cache-size: 100
  # unicast rate limiters config
  # Setting this to true will disable connection disconnects and gating when unicast rate limiters are configured
  unicast-dry-run: true
  # The number of seconds a peer will be forced to wait before being allowed to successfully reconnect to the node after being rate limited
  unicast-lockout-duration: 10s
  # Amount of unicast messages that can be sent by a peer per second
  unicast-message-rate-limit: 0
  # Bandwidth size in bytes a peer is allowed to send via unicast streams per second
  unicast-bandwidth-rate-limit: 0
  # Bandwidth size in bytes a peer is allowed to send via unicast streams at once
  unicast-bandwidth-burst-limit: 1e9
  # The minimum number of consecutive successful streams to reset the unicast stream creation retry budget from zero to the maximum default. If it is set to 100 for example, it
  # means that if a peer has 100 consecutive successful streams to the remote peer, and the remote peer has a zero stream creation budget,
  # the unicast stream creation retry budget for that remote peer will be reset to the maximum default.
  unicast-stream-zero-retry-reset-threshold: 100
  # The maximum number of retry attempts for creating a unicast stream to a remote peer before giving up. If it is set to 3 for example, it means that if a peer fails to create
  # retry a unicast stream to a remote peer 3 times, the peer will give up and will not retry creating a unicast stream to that remote peer.
  # When it is set to zero it means that the peer will not retry creating a unicast stream to a remote peer if it fails.
  unicast-max-stream-creation-retry-attempt-times: 3
  # The number of seconds that the local peer waits since the last successful dial to a remote peer before resetting the unicast dial retry budget from zero to the maximum default.
  # If it is set to 3600s (1h) for example, it means that if it has passed at least one hour since the last successful dial, and the remote peer has a zero dial retry budget,
  # the unicast dial retry budget for that remote peer will be reset to the maximum default.
  unicast-dial-zero-retry-reset-threshold: 3600s
  # The maximum number of retry attempts for dialing a remote peer before giving up. If it is set to 3 for example, it means that if a peer fails to dial a remote peer 3 times,
  # the peer will give up and will not retry dialing that remote peer.
  unicast-max-dial-retry-attempt-times: 3
  # The backoff delay used in the exponential backoff for consecutive failed unicast dial attempts to a remote peer.
  unicast-dial-backoff-delay: 1s
  # The backoff delay used in the exponential backoff for backing off concurrent create stream attempts to the same remote peer
  # when there is no available connections to that remote peer and a dial is in progress.
  unicast-dial-in-progress-backoff-delay: 1s
  # The size of the dial config cache used to keep track of the dial config for each remote peer. The dial config is used to keep track of the dial retry budget for each remote peer.
  # Recommended to set it to the maximum number of remote peers in the network.
  unicast-dial-config-cache-size: 10_000
  # Resource manager config
  # Maximum allowed fraction of file descriptors to be allocated by the libp2p resources in (0,1]
  libp2p-memory-limit-ratio: 0.5 # flow default
  # Maximum allowed fraction of memory to be allocated by the libp2p resources in (0,1]
  libp2p-file-descriptors-ratio: 0.2 # libp2p default
  # The default value for libp2p PeerBaseLimitConnsInbound. This limit
  # restricts the amount of inbound connections from each remote peer, forcing libp2p to reuse the connection.
  # We set this limit to zero by default to allow libp2p resource manager default limit value to be used.
  libp2p-peer-base-limits-conns-inbound: 0
  # maximum number of inbound system-wide streams, across all peers and protocols
  # Note that streams are ephemeral and are created and destroyed intermittently.
  libp2p-inbound-stream-limit-system: 15_000
  # maximum number of inbound transient streams, across all streams that are not yet fully opened and associated with a protocol.
  # Note that streams are ephemeral and are created and destroyed intermittently.
  libp2p-inbound-stream-limit-transient: 15_000
  # maximum number of inbound streams for each protocol across all peers; this is a per-protocol limit. We expect at least
  # three protocols per node; gossipsub, unicast, and dht. Note that streams are ephemeral and are created and destroyed intermittently.
  libp2p-inbound-stream-limit-protocol: 5000
  # maximum number of inbound streams from each peer across all protocols.
  libp2p-inbound-stream-limit-peer: 1000
  # maximum number of inbound streams from each peer for each protocol.
  libp2p-inbound-stream-limit-protocol-peer: 500
  # Connection manager config
  # HighWatermark and LowWatermark govern the number of connections are maintained by the ConnManager.
  # When the peer count exceeds the HighWatermark, as many peers will be pruned (and
  # their connections terminated) until LowWatermark peers remain. In other words, whenever the
  # peer count is x > HighWatermark, the ConnManager will prune x - LowWatermark peers.
  # The pruning algorithm is as follows:
  # 1. The ConnManager will not prune any peers that have been connected for less than GracePeriod.
  # 2. The ConnManager will not prune any peers that are protected.
  # 3. The ConnManager will sort the peers based on their number of streams and direction of connections, and
  # prunes the peers with the least number of streams. If there are ties, the peer with the incoming connection
  # will be pruned. If both peers have incoming connections, and there are still ties, one of the peers will be
  # pruned at random.
  # Algorithm implementation is in https://github.com/libp2p/go-libp2p/blob/master/p2p/net/connmgr/connmgr.go#L262-L318
  libp2p-high-watermark: 500
  libp2p-low-watermark: 450
  # The time to wait before pruning a new connection
  libp2p-silence-period: 10s
  # The time to wait before start pruning connections
  libp2p-grace-period: 1m
  # Gossipsub config
  # The default interval at which the mesh tracer logs the mesh topology. This is used for debugging and forensics purposes.
  #	Note that we purposefully choose this logging interval high enough to avoid spamming the logs. Moreover, the
  #	mesh updates will be logged individually and separately. The logging interval is only used to log the mesh
  #	topology as a whole specially when there are no updates to the mesh topology for a long time.
  gossipsub-local-mesh-logging-interval: 1m
  # The default interval at which the gossipsub score tracer logs the peer scores. This is used for debugging and forensics purposes.
  #	Note that we purposefully choose this logging interval high enough to avoid spamming the logs.
  gossipsub-score-tracer-interval: 1m
  # The default RPC sent tracker cache size. The RPC sent tracker is used to track RPC control messages sent from the local node.
  # Note: this cache size must be large enough to keep a history of sent messages in a reasonable time window of past history.
  gossipsub-rpc-sent-tracker-cache-size: 1_000_000
  # Cache size of the rpc sent tracker queue used for async tracking.
  gossipsub-rpc-sent-tracker-queue-cache-size: 100_000
  # Number of workers for rpc sent tracker worker pool.
  gossipsub-rpc-sent-tracker-workers: 5
  # Peer scoring is the default value for enabling peer scoring
  gossipsub-peer-scoring-enabled: true

  # Gossipsub rpc inspectors configs
  # The size of the queue for notifications about invalid RPC messages
  gossipsub-rpc-inspector-notification-cache-size: 10_000
  # RPC control message validation inspector configs
  # Rpc validation inspector number of pool workers
  gossipsub-rpc-validation-inspector-workers: 5
  # Max number of ihave messages in a sample to be inspected. If the number of ihave messages exceeds this configured value
  # the control message ihaves will be truncated to the max sample size. This sample is randomly selected.
  gossipsub-rpc-ihave-max-sample-size: 1000
  # Max number of ihave message ids in a sample to be inspected per ihave. Each ihave message includes a list of message ids
  # each. If the size of the message ids list for a single ihave message exceeds the configured max message id sample size the list of message ids will be truncated.
  gossipsub-rpc-ihave-max-message-id-sample-size: 1000
  # Max number of control messages in a sample to be inspected when inspecting GRAFT and PRUNE message types. If the total number of control messages (GRAFT or PRUNE)
  # exceeds this max sample size then the respective message will be truncated before being processed.
  gossipsub-rpc-graft-and-prune-message-max-sample-size: 1000
  # Max number of iwant messages in a sample to be inspected. If the total number of iWant control messages
  # exceeds this max sample size then the respective message will be truncated before being processed.
  gossipsub-rpc-iwant-max-sample-size: 1000
  # Max number of iwant message ids in a sample to be inspected per iwant. Each iwant message includes a list of message ids
  # each, if the size of this list exceeds the configured max message id sample size the list of message ids will be truncated.
  gossipsub-rpc-iwant-max-message-id-sample-size: 1000
  # The allowed threshold of iWant messages received without a corresponding tracked iHave message that was sent. If the cache miss threshold is exceeded an
  # invalid control message notification is disseminated and the sender will be penalized.
  gossipsub-rpc-iwant-cache-miss-threshold: .5
  # The iWants size at which message id cache misses will be checked.
  gossipsub-rpc-iwant-cache-miss-check-size: 1000
  # The max allowed duplicate message IDs in a single iWant control message. If the duplicate message threshold is exceeded an invalid control message
  # notification is disseminated and the sender will be penalized.
  gossipsub-rpc-iwant-duplicate-message-id-threshold: .15
  # The size of the queue used by worker pool for the control message validation inspector
  gossipsub-rpc-validation-inspector-queue-cache-size: 100
  # Cluster prefixed control message validation configs
  # The size of the cache used to track the amount of cluster prefixed topics received by peers
  gossipsub-cluster-prefix-tracker-cache-size: 100
  # The decay val used for the geometric decay of cache counters used to keep track of cluster prefixed topics received by peers
  gossipsub-cluster-prefix-tracker-cache-decay: 0.99
  # The upper bound on the amount of cluster prefixed control messages that will be processed
  gossipsub-rpc-cluster-prefixed-hard-threshold: 100
  # Application layer spam prevention
  alsp-spam-record-cache-size: 1000
  alsp-spam-report-queue-size: 10_000
  alsp-disable-penalty: false
  alsp-heart-beat-interval: 1s

  # Base probability in [0,1] that's used in creating the final probability of creating a
  # misbehavior report for a BatchRequest message. This is why the word "base" is used in the name of this field,
  # since it's not the final probability and there are other factors that determine the final probability.
  # The reason for this is that we want to increase the probability of creating a misbehavior report for a large batch.
  # Create misbehavior report for about 0.2% of BatchRequest messages for normal batch requests (i.e. not too large)
  # The final batch request probability is calculated as follows:
  # batchRequestBaseProb * (len(batchRequest.BlockIDs) + 1) / synccore.DefaultConfig().MaxSize
  # Example 1 (small batch of block IDs) if the batch request is for 10 blocks IDs and batchRequestBaseProb is 0.01, then the probability of
  # creating a misbehavior report is:
  # batchRequestBaseProb * (10+1) / synccore.DefaultConfig().MaxSize
  # = 0.01 * 11 / 64 = 0.00171875 = 0.171875%
  # Example 2 (large batch of block IDs) if the batch request is for 1000 block IDs and batchRequestBaseProb is 0.01, then the probability of
  # creating a misbehavior report is:
  # batchRequestBaseProb * (1000+1) / synccore.DefaultConfig().MaxSize
  # = 0.01 * 1001 / 64 = 0.15640625 = 15.640625%
  alsp-sync-engine-batch-request-base-prob: 0.01

  # Base probability in [0,1] that's used in creating the final probability of creating a
  # misbehavior report for a RangeRequest message. This is why the word "base" is used in the name of this field,
  # since it's not the final probability and there are other factors that determine the final probability.
  # The reason for this is that we want to increase the probability of creating a misbehavior report for a large range.
  # Create misbehavior report for about 0.2% of RangeRequest messages for normal range requests (i.e. not too large)
  # and about 15% of RangeRequest messages for very large range requests.
  # The final probability is calculated as follows:
  # rangeRequestBaseProb * ((rangeRequest.ToHeight-rangeRequest.FromHeight) + 1) / synccore.DefaultConfig().MaxSize
  # Example 1 (small range) if the range request is for 10 blocks and rangeRequestBaseProb is 0.01, then the probability of
  # creating a misbehavior report is:
  # rangeRequestBaseProb * (10+1) / synccore.DefaultConfig().MaxSize
  # = 0.01 * 11 / 64 = 0.00171875 = 0.171875%
  # Example 2 (large range) if the range request is for 1000 blocks and rangeRequestBaseProb is 0.01, then the probability of
  # creating a misbehavior report is:
  # rangeRequestBaseProb * (1000+1) / synccore.DefaultConfig().MaxSize
  # = 0.01 * 1001 / 64 = 0.15640625 = 15.640625%
  alsp-sync-engine-range-request-base-prob: 0.01

  # Probability in [0,1] of creating a misbehavior report for a SyncRequest message.
  # create misbehavior report for 1% of SyncRequest messages
  alsp-sync-engine-sync-request-prob: 0.01
