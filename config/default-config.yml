config-file: "./default-config.yml"
network-config:
  # Network Configuration
  # Connection pruning determines whether connections to nodes
  # that are not part of protocol state should be trimmed
  networking-connection-pruning: true
  # Preferred unicasts protocols list of unicast protocols in preferred order
  preferred-unicast-protocols: [ ]
  received-message-cache-size: 10_000
  peerupdate-interval: 10m
  unicast-message-timeout: 5s
  # Unicast create stream retry delay is initial delay used in the exponential backoff for create stream retries
  unicast-create-stream-retry-delay: 1s
  dns-cache-ttl: 5m
  # The size of the queue for notifications about new peers in the disallow list.
  disallow-list-notification-cache-size: 100
  # unicast rate limiters config
  # Setting this to true will disable connection disconnects and gating when unicast rate limiters are configured
  unicast-dry-run: true
  # The number of seconds a peer will be forced to wait before being allowed to successfully reconnect to the node after being rate limited
  unicast-lockout-duration: 10s
  # Amount of unicast messages that can be sent by a peer per second
  unicast-message-rate-limit: 0
  # Bandwidth size in bytes a peer is allowed to send via unicast streams per second
  unicast-bandwidth-rate-limit: 0
  # Bandwidth size in bytes a peer is allowed to send via unicast streams at once
  unicast-bandwidth-burst-limit: 1e9
  # The minimum number of consecutive successful streams to reset the unicast stream creation retry budget from zero to the maximum default. If it is set to 100 for example, it
  # means that if a peer has 100 consecutive successful streams to the remote peer, and the remote peer has a zero stream creation budget,
  # the unicast stream creation retry budget for that remote peer will be reset to the maximum default.
  unicast-stream-zero-retry-reset-threshold: 100
  # The maximum number of retry attempts for creating a unicast stream to a remote peer before giving up. If it is set to 3 for example, it means that if a peer fails to create
  # retry a unicast stream to a remote peer 3 times, the peer will give up and will not retry creating a unicast stream to that remote peer.
  # When it is set to zero it means that the peer will not retry creating a unicast stream to a remote peer if it fails.
  unicast-max-stream-creation-retry-attempt-times: 3
  # The size of the dial config cache used to keep track of the dial config for each remote peer. The dial config is used to keep track of the dial retry budget for each remote peer.
  # Recommended to set it to the maximum number of remote peers in the network.
  unicast-dial-config-cache-size: 10_000
  # Resource manager config
  libp2p-resource-manager:
    # Maximum allowed fraction of file descriptors to be allocated by the libp2p resources in [0,1]
    # setting to zero means no allocation of memory by libp2p; and libp2p will run with very low limits
    memory-limit-ratio: 0.5 # flow default
    # Maximum allowed fraction of memory to be allocated by the libp2p resources in [0,1]
    # setting to zero means no allocation of memory by libp2p; and libp2p will run with very low limits
    file-descriptors-ratio: 0.2 # libp2p default
    # limits override: any non-zero values for libp2p-resource-limit-override will override the default values of the libp2p resource limits.
    limits-override:
      system:
        # maximum number of inbound system-wide streams, across all peers and protocols
        # Note that streams are ephemeral and are created and destroyed intermittently.
        streams-inbound: 15_000 # override
        # maximum number of outbound system-wide streams, across all peers and protocols
        # Note that streams are ephemeral and are created and destroyed intermittently.
        streams-outbound: 15_000 # override
        connections-inbound: 0 # no-override, use default
        connections-outbound: 0 # no-override, use default
        fd: 0 # no-override, use default
        memory-bytes: 0 # no-override, use default
      transient:
        # maximum number of inbound transient streams, across all streams that are not yet fully opened and associated with a protocol.
        # Note that streams are ephemeral and are created and destroyed intermittently.
        streams-inbound: 15_000 # override
        # maximum number of outbound transient streams, across all streams that are not yet fully opened and associated with a protocol.
        # Note that streams are ephemeral and are created and destroyed intermittently.
        streams-outbound: 15_000 # override
        connections-inbound: 0 # no-override, use default
        connections-outbound: 0 # no-override, use default
        fd: 0 # no-override, use default
        memory-bytes: 0 # no-override, use default
      protocol:
        # maximum number of inbound streams for each protocol across all peers; this is a per-protocol limit. We expect at least
        # three protocols per node; gossipsub, unicast, and dht. Note that streams are ephemeral and are created and destroyed intermittently.
        streams-inbound: 5000 # override
        # maximum number of outbound streams for each protocol across all peers; this is a per-protocol limit. We expect at least
        # three protocols per node; gossipsub, unicast, and dht. Note that streams are ephemeral and are created and destroyed intermittently.
        streams-outbound: 5000 # override
        connections-inbound: 0 # no-override, use default
        connections-outbound: 0 # no-override, use default
        fd: 0 # no-override, use default
        memory-bytes: 0 # no-override, use default
      peer:
        # maximum number of inbound streams from each peer across all protocols.
        streams-inbound: 1000 # override
        # maximum number of outbound streams from each peer across all protocols.
        streams-outbound: 1000 # override
        connections-inbound: 0 # no-override, use default
        connections-outbound: 0 # no-override, use default
        fd: 0 # no-override, use default
        memory-bytes: 0 # no-override, use default
      peer-protocol:
        # maximum number of inbound streams from each peer for each protocol.
        streams-inbound: 500 # override
        # maximum number of outbound streams from each peer for each protocol.
        streams-outbound: 500 # override
        connections-inbound: 0 # no-override, use default
        connections-outbound: 0 # no-override, use default
        fd: 0 # no-override, use default
        memory-bytes: 0 # no-override, use default
  # Connection manager config
  # HighWatermark and LowWatermark govern the number of connections are maintained by the ConnManager.
  # When the peer count exceeds the HighWatermark, as many peers will be pruned (and
  # their connections terminated) until LowWatermark peers remain. In other words, whenever the
  # peer count is x > HighWatermark, the ConnManager will prune x - LowWatermark peers.
  # The pruning algorithm is as follows:
  # 1. The ConnManager will not prune any peers that have been connected for less than GracePeriod.
  # 2. The ConnManager will not prune any peers that are protected.
  # 3. The ConnManager will sort the peers based on their number of streams and direction of connections, and
  # prunes the peers with the least number of streams. If there are ties, the peer with the incoming connection
  # will be pruned. If both peers have incoming connections, and there are still ties, one of the peers will be
  # pruned at random.
  # Algorithm implementation is in https://github.com/libp2p/go-libp2p/blob/master/p2p/net/connmgr/connmgr.go#L262-L318
  libp2p-high-watermark: 500
  libp2p-low-watermark: 450
  # The time to wait before pruning a new connection
  libp2p-silence-period: 10s
  # The time to wait before start pruning connections
  libp2p-grace-period: 1m
  # Gossipsub config
  # The default interval at which the mesh tracer logs the mesh topology. This is used for debugging and forensics purposes.
  #	Note that we purposefully choose this logging interval high enough to avoid spamming the logs. Moreover, the
  #	mesh updates will be logged individually and separately. The logging interval is only used to log the mesh
  #	topology as a whole specially when there are no updates to the mesh topology for a long time.
  gossipsub-local-mesh-logging-interval: 1m
  # The default interval at which the gossipsub score tracer logs the peer scores. This is used for debugging and forensics purposes.
  #	Note that we purposefully choose this logging interval high enough to avoid spamming the logs.
  gossipsub-score-tracer-interval: 1m
  # The default RPC sent tracker cache size. The RPC sent tracker is used to track RPC control messages sent from the local node.
  # Note: this cache size must be large enough to keep a history of sent messages in a reasonable time window of past history.
  gossipsub-rpc-sent-tracker-cache-size: 1_000_000
  # Cache size of the rpc sent tracker queue used for async tracking.
  gossipsub-rpc-sent-tracker-queue-cache-size: 100_000
  # Number of workers for rpc sent tracker worker pool.
  gossipsub-rpc-sent-tracker-workers: 5
  # Peer scoring is the default value for enabling peer scoring
  gossipsub-peer-scoring-enabled: true
  # The interval for updating the list of subscribed peers to all topics in gossipsub. This is used to keep track of subscriptions
  # violations and penalize peers accordingly. Recommended value is in the order of a few minutes to avoid contentions; as the operation
  # reads all topics and all peers subscribed to each topic.
  gossipsub-subscription-provider-update-interval: 10m
  # The size of cache for keeping the list of all peers subscribed to each topic (same as the local node). This cache is the local node's
  # view of the network and is used to detect subscription violations and penalize peers accordingly. Recommended to be big enough to
  # keep the entire network's size. Otherwise, the local node's view of the network will be incomplete due to cache eviction.
  # Recommended size is 10x the number of peers in the network.
  gossipsub-subscription-provider-cache-size: 10000

  # Gossipsub rpc inspectors configs
  # The size of the queue for notifications about invalid RPC messages
  gossipsub-rpc-inspector-notification-cache-size: 10_000
  # RPC control message validation inspector configs
  # Rpc validation inspector number of pool workers
  gossipsub-rpc-validation-inspector-workers: 5
  # Max number of ihave messages in a sample to be inspected. If the number of ihave messages exceeds this configured value
  # the control message ihaves will be truncated to the max sample size. This sample is randomly selected.
  gossipsub-rpc-ihave-max-sample-size: 1000
  # Max number of ihave message ids in a sample to be inspected per ihave. Each ihave message includes a list of message ids
  # each. If the size of the message ids list for a single ihave message exceeds the configured max message id sample size the list of message ids will be truncated.
  gossipsub-rpc-ihave-max-message-id-sample-size: 1000
  # Max number of control messages in a sample to be inspected when inspecting GRAFT and PRUNE message types. If the total number of control messages (GRAFT or PRUNE)
  # exceeds this max sample size then the respective message will be truncated before being processed.
  gossipsub-rpc-graft-and-prune-message-max-sample-size: 1000
  # Max number of iwant messages in a sample to be inspected. If the total number of iWant control messages
  # exceeds this max sample size then the respective message will be truncated before being processed.
  gossipsub-rpc-iwant-max-sample-size: 1000
  # Max number of iwant message ids in a sample to be inspected per iwant. Each iwant message includes a list of message ids
  # each, if the size of this list exceeds the configured max message id sample size the list of message ids will be truncated.
  gossipsub-rpc-iwant-max-message-id-sample-size: 1000
  # The allowed threshold of iWant messages received without a corresponding tracked iHave message that was sent. If the cache miss threshold is exceeded an
  # invalid control message notification is disseminated and the sender will be penalized.
  gossipsub-rpc-iwant-cache-miss-threshold: .5
  # The iWants size at which message id cache misses will be checked.
  gossipsub-rpc-iwant-cache-miss-check-size: 1000
  # The max allowed duplicate message IDs in a single iWant control message. If the duplicate message threshold is exceeded an invalid control message
  # notification is disseminated and the sender will be penalized.
  gossipsub-rpc-iwant-duplicate-message-id-threshold: .15
  # The size of the queue used by worker pool for the control message validation inspector
  gossipsub-rpc-validation-inspector-queue-cache-size: 100
  # Cluster prefixed control message validation configs
  # The size of the cache used to track the amount of cluster prefixed topics received by peers
  gossipsub-cluster-prefix-tracker-cache-size: 100
  # The decay val used for the geometric decay of cache counters used to keep track of cluster prefixed topics received by peers
  gossipsub-cluster-prefix-tracker-cache-decay: 0.99
  # The upper bound on the amount of cluster prefixed control messages that will be processed
  gossipsub-rpc-cluster-prefixed-hard-threshold: 100
  # The max sample size used for RPC message validation. If the total number of RPC messages exceeds this value a sample will be taken but messages will not be truncated
  gossipsub-rpc-message-max-sample-size: 1000
  # The threshold at which an error will be returned if the number of invalid RPC messages exceeds this value
  gossipsub-rpc-message-error-threshold: 500
  # RPC metrics observer inspector configs
  # The number of metrics inspector pool workers
  gossipsub-rpc-metrics-inspector-workers: 1
  # The size of the queue used by worker pool for the control message metrics inspector
  gossipsub-rpc-metrics-inspector-cache-size: 100
  # The penalty values for each gossipsub RPC control message type
  gossipsub-score-penalty:
    graft: -0.01
    prune: -0.01
    iHave: -0.01
    iWant: -0.01
    publish: -0.01
    cluster-prefixed-reduction-factor: 0.2
  # Threshold level for penalty. At each evaluation period, when a node's penalty is below this value, the decay rate slows down, ensuring longer decay periods for malicious nodes and quicker decay for honest ones.
  gossipsub-app-specific-penalty-decay-slowdown-threshold: -99
  # This setting adjusts the decay rate when a node's penalty falls below the threshold. The decay rate, ranging between 0 and 1, dictates how quickly penalties decrease: a higher rate results in slower decay. The decay calculation is multiplicative (newPenalty = decayRate * oldPenalty). The reduction factor increases the decay rate, thus decelerating the penalty reduction. For instance, with a 0.01 reduction factor, the decay rate increases by 0.01 at each evaluation interval when the penalty is below the threshold. Consequently, a decay rate of `x` diminishes the penalty to zero more rapidly than a rate of `x+0.01`.
  gossipsub-app-specific-penalty-decay-rate-reduction-factor: .01
  # Defines the frequency for evaluating and potentially adjusting the decay process of a spam record. At each interval, the system assesses the current penalty of a node. If this penalty is below the defined threshold, the decay rate is modified according to the reduction factor, slowing down the penalty reduction process. This reassessment at regular intervals ensures that the decay rate is dynamically adjusted to reflect the node's ongoing behavior, maintaining a balance between penalizing malicious activity and allowing recovery for honest nodes.
  gossipsub-app-specific-penalty-decay-evaluation-period: 10m
  # Application layer spam prevention
  alsp-spam-record-cache-size: 1000
  alsp-spam-report-queue-size: 10_000
  alsp-disable-penalty: false
  alsp-heart-beat-interval: 1s

  # Base probability in [0,1] that's used in creating the final probability of creating a
  # misbehavior report for a BatchRequest message. This is why the word "base" is used in the name of this field,
  # since it's not the final probability and there are other factors that determine the final probability.
  # The reason for this is that we want to increase the probability of creating a misbehavior report for a large batch.
  # Create misbehavior report for about 0.2% of BatchRequest messages for normal batch requests (i.e. not too large)
  # The final batch request probability is calculated as follows:
  # batchRequestBaseProb * (len(batchRequest.BlockIDs) + 1) / synccore.DefaultConfig().MaxSize
  # Example 1 (small batch of block IDs) if the batch request is for 10 blocks IDs and batchRequestBaseProb is 0.01, then the probability of
  # creating a misbehavior report is:
  # batchRequestBaseProb * (10+1) / synccore.DefaultConfig().MaxSize
  # = 0.01 * 11 / 64 = 0.00171875 = 0.171875%
  # Example 2 (large batch of block IDs) if the batch request is for 1000 block IDs and batchRequestBaseProb is 0.01, then the probability of
  # creating a misbehavior report is:
  # batchRequestBaseProb * (1000+1) / synccore.DefaultConfig().MaxSize
  # = 0.01 * 1001 / 64 = 0.15640625 = 15.640625%
  alsp-sync-engine-batch-request-base-prob: 0.01

  # Base probability in [0,1] that's used in creating the final probability of creating a
  # misbehavior report for a RangeRequest message. This is why the word "base" is used in the name of this field,
  # since it's not the final probability and there are other factors that determine the final probability.
  # The reason for this is that we want to increase the probability of creating a misbehavior report for a large range.
  # Create misbehavior report for about 0.2% of RangeRequest messages for normal range requests (i.e. not too large)
  # and about 15% of RangeRequest messages for very large range requests.
  # The final probability is calculated as follows:
  # rangeRequestBaseProb * ((rangeRequest.ToHeight-rangeRequest.FromHeight) + 1) / synccore.DefaultConfig().MaxSize
  # Example 1 (small range) if the range request is for 10 blocks and rangeRequestBaseProb is 0.01, then the probability of
  # creating a misbehavior report is:
  # rangeRequestBaseProb * (10+1) / synccore.DefaultConfig().MaxSize
  # = 0.01 * 11 / 64 = 0.00171875 = 0.171875%
  # Example 2 (large range) if the range request is for 1000 blocks and rangeRequestBaseProb is 0.01, then the probability of
  # creating a misbehavior report is:
  # rangeRequestBaseProb * (1000+1) / synccore.DefaultConfig().MaxSize
  # = 0.01 * 1001 / 64 = 0.15640625 = 15.640625%
  alsp-sync-engine-range-request-base-prob: 0.01

  # Probability in [0,1] of creating a misbehavior report for a SyncRequest message.
  # create misbehavior report for 1% of SyncRequest messages
  alsp-sync-engine-sync-request-prob: 0.01
